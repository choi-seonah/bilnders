{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#google drive와 colab 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCmQf7eaAyIg",
        "outputId": "d1f994bd-d6c3-4049-c3d7-f5c9e031ed96"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXeW4A17_bZZ",
        "outputId": "53b5354a-1466-432a-af03-2bbba851aa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display"
      ],
      "metadata": {
        "id": "HoCHu5kR_ec-"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coarse_label 작업"
      ],
      "metadata": {
        "id": "Xg2r8R7S6xvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#movenet lighting 모델 가져오기\n",
        "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ],
      "metadata": {
        "cellView": "code",
        "id": "yHx_wdn2_nUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#annotation할 데이터 가져오기\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 마운트시킨 데이터 셋 위치\n",
        "data_path = '/content/gdrive/MyDrive/Tadasana'\n",
        "classes = [path for path in Path(data_path).iterdir() if path.is_dir()]\n",
        "classes"
      ],
      "metadata": {
        "id": "vYOQaIkWBLtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "for cls in classes:\n",
        "  files += [x.as_posix() for x in Path(cls).glob('**/*') if x.is_file()]\n",
        "files"
      ],
      "metadata": {
        "id": "stEmFeOE1fVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inference하고 json으로 저장\n",
        "def get_keypoints(image,\n",
        "                  keypoints_with_scores,\n",
        "                  output_image_height=None,\n",
        "                  keypoint_threshold=0.0):\n",
        "    height, width, channel = image.shape\n",
        "    aspect_ratio = float(width) / height\n",
        "\n",
        "    keypoints_all = []\n",
        "    num_instances,_,_,_ = keypoints_with_scores.shape\n",
        "    for id in range(num_instances):\n",
        "        kpts_x = keypoints_with_scores[0,id,:,1]\n",
        "        kpts_y = keypoints_with_scores[0,id,:,0]\n",
        "        kpts_scores = keypoints_with_scores[0,id,:,2]\n",
        "        kpts_abs_xy = np.stack(\n",
        "            [width*np.array(kpts_x),height*np.array(kpts_y)],axis=-1)\n",
        "        kpts_above_thrs_abs = kpts_abs_xy[kpts_scores > keypoint_threshold,: ]\n",
        "        keypoints_all.append(kpts_above_thrs_abs)\n",
        "\n",
        "    return np.concatenate(keypoints_all,axis=0)"
      ],
      "metadata": {
        "id": "Pp1UF30QBo3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#{'file path' : (17, 2) 배열} 상태로 딕셔너리에 저장\n",
        "import json\n",
        "\n",
        "keypoints = {}\n",
        "\n",
        "for image_path in files:\n",
        "    # 이미지 에러에 대한 처리\n",
        "    # 이미지가 tf.io.read_file로 읽을 수 없는 타입인 경우에 대비\n",
        "    try:\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image)\n",
        "    except:\n",
        "        print('image error : ', image_path)\n",
        "        continue\n",
        "\n",
        "    input_image = tf.expand_dims(image, axis=0)\n",
        "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "\n",
        "    # 모델 인퍼런스 에러에 대한 처리\n",
        "    try:\n",
        "        keypoints_with_scores = movenet(input_image)\n",
        "    except:\n",
        "        print('model error : ', image_path)\n",
        "        continue\n",
        "\n",
        "    display_image = tf.expand_dims(image, axis=0)\n",
        "    display_image = tf.cast(tf.image.resize_with_pad(\n",
        "        display_image, 224, 224), dtype=tf.int32)\n",
        "    output_overlay = get_keypoints(np.squeeze(display_image.numpy(), axis=0),\n",
        "                                  keypoints_with_scores)\n",
        "\n",
        "    keypoints.setdefault('/'.join(image_path.split('/')[3:]), output_overlay.tolist())\n",
        "\n",
        "keypoints"
      ],
      "metadata": {
        "id": "RlC6C2C4Bpde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#명령어 실행 시 json 파일 생성된 것 확인 가능\n",
        "with open(\"./keypoints.json\", \"w\") as json_file:\n",
        "    json.dump(keypoints, json_file)"
      ],
      "metadata": {
        "id": "d8IP3rN8B8xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#명령어 실행 시 로컬 PC에 파일 저장 가능\n",
        "from google.colab import files\n",
        "files.download(\"./keypoints.json\")"
      ],
      "metadata": {
        "id": "BFSngNDlCGUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 데이터와 keypoints 데이터를 로드하는 데이터로더 만들기\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"./keypoints.json\", \"r\") as json_file:\n",
        "    keypoint_dict = json.load(json_file)\n",
        "\n",
        "new_keypoint_dict = {}\n",
        "for key in keypoint_dict.keys():\n",
        "    new_key = '/'.join(key.split('/')[-2:])\n",
        "    new_val = keypoint_dict[key]\n",
        "    new_keypoint_dict[new_key] = new_val\n",
        "\n",
        "del keypoint_dict\n",
        "keypoint_dict = new_keypoint_dict\n",
        "del new_keypoint_dict\n",
        "\n",
        "def process_keypoint(file_path):\n",
        "    file_path = '/'.join(file_path.numpy().decode('utf-8').split('/')[-2:])\n",
        "    try:\n",
        "        keypoint = tf.convert_to_tensor(keypoint_dict[file_path], dtype=tf.float32)\n",
        "    except KeyError:\n",
        "        print(f\"KeyError: The key '{file_path}' not found in keypoint_dict.\")\n",
        "        # 기본 키포인트 값 설정 (예: 모든 값이 0인 배열)\n",
        "        keypoint = tf.zeros([17, 2], dtype=tf.float32)  # 17x2는 예시입니다. 실제 키포인트 차원에 맞게 조정하세요.\n",
        "    return keypoint\n",
        "\n",
        "\n",
        "def process_path(file_path, class_names, img_shape=(224, 224)):\n",
        "    label = tf.strings.split(file_path, os.path.sep)[-2]\n",
        "    label_index = tf.argmax(class_names == label)\n",
        "    label = tf.one_hot(label_index, depth=len(class_names))\n",
        "\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, img_shape)\n",
        "\n",
        "    [keypoint,] = tf.py_function(process_keypoint, [file_path], [tf.float32])\n",
        "    return {\"image_input\": img, \"keypoints_input\": keypoint}, label\n",
        "\n",
        "\n",
        "def load_label(label_path):\n",
        "    class_names = []\n",
        "    with open(label_path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            class_names.append(line)\n",
        "\n",
        "    return np.array(class_names)\n",
        "\n",
        "\n",
        "def get_spilt_data(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=1000):\n",
        "    assert (train_split + val_split) == 1\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "\n",
        "    train_ds = ds.take(train_size)\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "\n",
        "def augment(inputs, label):\n",
        "    image, keypoint = inputs['image_input'], inputs['keypoints_input']\n",
        "    image = tf.image.random_crop(image, size=[224, 224, 3])\n",
        "    image = tf.image.adjust_brightness(image, 0.4)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.4)\n",
        "    return {'image_input' : image, 'keypoints_input' : keypoint}, label\n",
        "\n",
        "\n",
        "def prepare_for_training(ds, batch_size=32, cache=True, training=True):\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            ds = ds.cache(cache)\n",
        "        else:\n",
        "            ds = ds.cache()\n",
        "\n",
        "    ds = ds.repeat()\n",
        "    if training:\n",
        "        ds = ds.map(lambda x, y: augment(x, y))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "#load_data의 역할 : 라벨 로드, 이미지 로드, 데이터 배치 만들기 등의 역할을 하는 함수를 호출하여 최종으로 받을 데이터를 만드는 역할\n",
        "def load_data(data_path, img_shape, batch_size=32, is_train=True):\n",
        "    class_names = [cls for cls in os.listdir(data_path) if cls != '.DS_Store']\n",
        "    data_dir = pathlib.Path(data_path)\n",
        "    list_ds = tf.data.Dataset.list_files(str(data_dir / '*/*'))\n",
        "\n",
        "    labeled_ds = list_ds.map(lambda x: process_path(x, class_names, img_shape))\n",
        "    labeled_ds = prepare_for_training(labeled_ds, batch_size=batch_size, training=is_train)\n",
        "\n",
        "    DATASET_SIZE = tf.data.experimental.cardinality(list_ds).numpy()\n",
        "\n",
        "    return labeled_ds, DATASET_SIZE"
      ],
      "metadata": {
        "id": "1gPHAkoSCLaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi-input 모델 만들기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "class YogaPose(tf.keras.Model):\n",
        "    def __init__(self, num_classes=30, freeze=False):\n",
        "        super(YogaPose, self).__init__()\n",
        "        self.base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "        self.keypoint = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(17, 2)),\n",
        "                                              tf.keras.layers.Dense(34),])\n",
        "\n",
        "        if freeze:\n",
        "            self.base_model.trainable = False\n",
        "\n",
        "        self.top = tf.keras.Sequential([tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n",
        "                                        tf.keras.layers.BatchNormalization(),\n",
        "                                        tf.keras.layers.Dropout(0.6, name=\"top_dropout\")])\n",
        "\n",
        "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        image, keypoint = inputs['image_input'], inputs['keypoints_input']\n",
        "        x1 = self.base_model(image)\n",
        "        x1 = self.top(x1)\n",
        "        x2 = self.keypoint(keypoint)\n",
        "        x = self.concat([x1, x2])\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-wJmMlzj6KiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 경로\n",
        "json_file_path = '/content/gdrive/MyDrive/dataset/keypoints.json'\n",
        "\n",
        "# JSON 파일 로드 및 Python 딕셔너리로 변환\n",
        "with open(json_file_path, 'r') as file:\n",
        "    keypoints_dict = json.load(file)\n",
        "\n",
        "# keypoints_dict는 이제 JSON 파일의 내용을 담고 있는 Python 딕셔너리입니다.\n"
      ],
      "metadata": {
        "id": "65pyEUPaAzKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 생성"
      ],
      "metadata": {
        "id": "niHN-zG061uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate\n",
        "\n",
        "# 모델 정의\n",
        "def create_model(input_shape=(224, 224, 3), num_classes=27):  # 클래스 수를 데이터셋에 맞게 조정\n",
        "    # 이미지 입력\n",
        "    image_input = Input(shape=input_shape, name='image_input')\n",
        "\n",
        "    # CNN 레이어\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Keypoints 입력 (실제 키포인트 차원에 맞게 조정 필요)\n",
        "    keypoints_input = Input(shape=(34,), name='keypoints_input')  # 34는 예시입니다.\n",
        "\n",
        "    # Concatenate 이미지 특징과 keypoints\n",
        "    concatenated = Concatenate()([x, keypoints_input])\n",
        "\n",
        "    # Dense 레이어\n",
        "    x = Dense(128, activation='relu')(concatenated)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = Model(inputs=[image_input, keypoints_input], outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = create_model(input_shape=(224, 224, 3), num_classes=27)  # 클래스 수에 맞게 조정\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 데이터 로드 및 모델 학습 부분은 동일하게 유지\n"
      ],
      "metadata": {
        "id": "OwuCkXt263AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습"
      ],
      "metadata": {
        "id": "3xFT7WSsEd72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 32\n",
        "train_ds, train_size = load_data(data_path=train_data_path, img_shape=(224, 224), batch_size=batch_size)\n",
        "\n",
        "# 데이터셋 크기 확인 및 steps_per_epoch 계산\n",
        "if train_size > 0:\n",
        "    steps_per_epoch = train_size // batch_size\n",
        "    if train_size % batch_size != 0:\n",
        "        steps_per_epoch += 1\n",
        "else:\n",
        "    raise ValueError(\"Training dataset is empty. Check your dataset and loading logic.\")\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 10\n",
        "history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
        "\n",
        "# 모델 저장\n",
        "model.save('path_to_my_model.h5')\n"
      ],
      "metadata": {
        "id": "8O3Os4a2Ee13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}